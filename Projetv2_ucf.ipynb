{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkCAx1Oa7S-D",
        "outputId": "55f7472b-c2b5-4ad9-c77c-38612ccdb7f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\calorrain\\appdata\\local\\anaconda3\\envs\\visu\\lib\\site-packages (from transformers) (1.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\calorrain\\appdata\\local\\anaconda3\\envs\\visu\\lib\\site-packages (from transformers) (25.0)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached PyYAML-6.0.2-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
            "Collecting requests (from transformers)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\calorrain\\appdata\\local\\anaconda3\\envs\\visu\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\calorrain\\appdata\\local\\anaconda3\\envs\\visu\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
            "  Downloading charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 10.4/10.4 MB 107.7 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "Using cached PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
            "Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
            "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.4/2.4 MB 135.8 MB/s eta 0:00:00\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl (102 kB)\n",
            "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 idna-3.10 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3 urllib3-2.4.0\n",
            "Collecting av\n",
            "  Downloading av-14.2.0-cp39-cp39-win_amd64.whl.metadata (4.8 kB)\n",
            "Downloading av-14.2.0-cp39-cp39-win_amd64.whl (30.9 MB)\n",
            "   ---------------------------------------- 0.0/30.9 MB ? eta -:--:--\n",
            "   ------------------- -------------------- 15.2/30.9 MB 79.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 30.9/30.9 MB 81.6 MB/s eta 0:00:00\n",
            "Installing collected packages: av\n",
            "Successfully installed av-14.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install av"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XU_-YcBz7Yf4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import av\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ksLuqkqm7ZDw"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# os.mkdir('results')\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq0RsySA7c1b",
        "outputId": "5e72588d-5284-4786-fd10-fd79fd483e2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0 6611M    0 7711k    0     0  12.6M      0  0:08:41 --:--:--  0:08:41 12.6M\n",
            "  0 6611M    0 37.3M    0     0  23.4M      0  0:04:41  0:00:01  0:04:40 23.5M\n",
            "  1 6611M    1 80.8M    0     0  31.2M      0  0:03:31  0:00:02  0:03:29 31.2M\n",
            "  2 6611M    2  136M    0     0  38.0M      0  0:02:53  0:00:03  0:02:50 38.1M\n",
            "  2 6611M    2  187M    0     0  40.8M      0  0:02:41  0:00:04  0:02:37 40.8M\n",
            "  3 6611M    3  222M    0     0  39.8M      0  0:02:45  0:00:05  0:02:40 43.0M\n",
            "  3 6611M    3  259M    0     0  39.4M      0  0:02:47  0:00:06  0:02:41 44.5M\n",
            "  4 6611M    4  298M    0     0  39.3M      0  0:02:48  0:00:07  0:02:41 43.5M\n",
            "  5 6611M    5  338M    0     0  39.3M      0  0:02:47  0:00:08  0:02:39 40.2M\n",
            "  5 6611M    5  378M    0     0  39.4M      0  0:02:47  0:00:09  0:02:38 38.1M\n",
            "  6 6611M    6  418M    0     0  39.5M      0  0:02:47  0:00:10  0:02:37 39.1M\n",
            "  6 6611M    6  458M    0     0  39.5M      0  0:02:46  0:00:11  0:02:35 39.7M\n",
            "  7 6611M    7  499M    0     0  39.6M      0  0:02:46  0:00:12  0:02:34 40.1M\n",
            "  8 6611M    8  540M    0     0  39.7M      0  0:02:46  0:00:13  0:02:33 40.4M\n",
            "  8 6611M    8  580M    0     0  39.7M      0  0:02:46  0:00:14  0:02:32 40.4M\n",
            "  9 6611M    9  621M    0     0  39.8M      0  0:02:45  0:00:15  0:02:30 40.5M\n",
            " 10 6611M   10  662M    0     0  39.9M      0  0:02:45  0:00:16  0:02:29 40.6M\n",
            " 10 6611M   10  703M    0     0  40.0M      0  0:02:45  0:00:17  0:02:28 40.8M\n",
            " 11 6611M   11  746M    0     0  40.1M      0  0:02:44  0:00:18  0:02:26 41.3M\n",
            " 11 6611M   11  790M    0     0  40.3M      0  0:02:43  0:00:19  0:02:24 42.0M\n",
            " 12 6611M   12  836M    0     0  40.6M      0  0:02:42  0:00:20  0:02:22 43.1M\n",
            " 13 6611M   13  883M    0     0  40.8M      0  0:02:41  0:00:21  0:02:20 44.1M\n",
            " 13 6611M   13  921M    0     0  40.8M      0  0:02:42  0:00:22  0:02:20 43.6M\n",
            " 14 6611M   14  962M    0     0  40.7M      0  0:02:42  0:00:23  0:02:19 43.1M\n",
            " 15 6611M   15 1005M    0     0  40.8M      0  0:02:41  0:00:24  0:02:17 42.9M\n",
            " 15 6611M   15 1051M    0     0  41.0M      0  0:02:40  0:00:25  0:02:15 42.8M\n",
            " 16 6611M   16 1094M    0     0  41.1M      0  0:02:40  0:00:26  0:02:14 42.3M\n",
            " 17 6611M   17 1129M    0     0  40.9M      0  0:02:41  0:00:27  0:02:14 41.5M\n",
            " 17 6611M   17 1166M    0     0  40.8M      0  0:02:41  0:00:28  0:02:13 40.9M\n",
            " 18 6611M   18 1205M    0     0  40.7M      0  0:02:42  0:00:29  0:02:13 39.9M\n",
            " 18 6611M   18 1244M    0     0  40.6M      0  0:02:42  0:00:30  0:02:12 38.7M\n",
            " 19 6611M   19 1284M    0     0  40.6M      0  0:02:42  0:00:31  0:02:11 38.0M\n",
            " 20 6611M   20 1325M    0     0  40.6M      0  0:02:42  0:00:32  0:02:10 39.0M\n",
            " 20 6611M   20 1365M    0     0  40.6M      0  0:02:42  0:00:33  0:02:09 39.7M\n",
            " 21 6611M   21 1405M    0     0  40.6M      0  0:02:42  0:00:34  0:02:08 40.1M\n",
            " 21 6611M   21 1446M    0     0  40.6M      0  0:02:42  0:00:35  0:02:07 40.3M\n",
            " 22 6611M   22 1486M    0     0  40.6M      0  0:02:42  0:00:36  0:02:06 40.4M\n",
            " 23 6611M   23 1527M    0     0  40.6M      0  0:02:42  0:00:37  0:02:05 40.4M\n",
            " 23 6611M   23 1568M    0     0  40.6M      0  0:02:42  0:00:38  0:02:04 40.6M\n",
            " 24 6611M   24 1610M    0     0  40.6M      0  0:02:42  0:00:39  0:02:03 40.8M\n",
            " 25 6611M   25 1653M    0     0  40.7M      0  0:02:42  0:00:40  0:02:02 41.3M\n",
            " 25 6611M   25 1696M    0     0  40.7M      0  0:02:42  0:00:41  0:02:01 41.8M\n",
            " 26 6611M   26 1729M    0     0  40.6M      0  0:02:42  0:00:42  0:02:00 40.4M\n",
            " 26 6611M   26 1766M    0     0  40.5M      0  0:02:43  0:00:43  0:02:00 39.5M\n",
            " 27 6611M   27 1805M    0     0  40.4M      0  0:02:43  0:00:44  0:01:59 38.9M\n",
            " 27 6611M   27 1846M    0     0  40.5M      0  0:02:43  0:00:45  0:01:58 38.7M\n",
            " 28 6611M   28 1889M    0     0  40.5M      0  0:02:43  0:00:46  0:01:57 38.6M\n",
            " 29 6611M   29 1924M    0     0  40.4M      0  0:02:43  0:00:47  0:01:56 38.9M\n",
            " 29 6611M   29 1957M    0     0  40.2M      0  0:02:44  0:00:48  0:01:56 38.1M\n",
            " 30 6611M   30 1991M    0     0  40.1M      0  0:02:44  0:00:49  0:01:55 37.2M\n",
            " 30 6611M   30 2027M    0     0  40.0M      0  0:02:44  0:00:50  0:01:54 36.1M\n",
            " 31 6611M   31 2063M    0     0  39.9M      0  0:02:45  0:00:51  0:01:54 34.8M\n",
            " 31 6611M   31 2099M    0     0  39.9M      0  0:02:45  0:00:52  0:01:53 35.1M\n",
            " 32 6611M   32 2126M    0     0  39.6M      0  0:02:46  0:00:53  0:01:53 33.8M\n",
            " 32 6611M   32 2155M    0     0  39.4M      0  0:02:47  0:00:54  0:01:53 32.6M\n",
            " 33 6611M   33 2184M    0     0  39.3M      0  0:02:48  0:00:55  0:01:53 31.5M\n",
            " 33 6611M   33 2215M    0     0  39.1M      0  0:02:48  0:00:56  0:01:52 30.4M\n",
            " 33 6611M   33 2246M    0     0  39.0M      0  0:02:49  0:00:57  0:01:52 29.3M\n",
            " 34 6611M   34 2277M    0     0  38.8M      0  0:02:50  0:00:58  0:01:52 30.2M\n",
            " 34 6611M   34 2308M    0     0  38.7M      0  0:02:50  0:00:59  0:01:51 30.7M\n",
            " 35 6611M   35 2339M    0     0  38.6M      0  0:02:51  0:01:00  0:01:51 31.0M\n",
            " 35 6611M   35 2371M    0     0  38.4M      0  0:02:51  0:01:01  0:01:50 31.1M\n",
            " 36 6611M   36 2400M    0     0  38.3M      0  0:02:52  0:01:02  0:01:50 30.8M\n",
            " 36 6611M   36 2424M    0     0  38.1M      0  0:02:53  0:01:03  0:01:50 29.4M\n",
            " 37 6611M   37 2451M    0     0  37.9M      0  0:02:54  0:01:04  0:01:50 28.5M\n",
            " 37 6611M   37 2479M    0     0  37.8M      0  0:02:54  0:01:05  0:01:49 27.8M\n",
            " 37 6611M   37 2508M    0     0  37.6M      0  0:02:55  0:01:06  0:01:49 27.5M\n",
            " 38 6611M   38 2538M    0     0  37.5M      0  0:02:56  0:01:07  0:01:49 27.6M\n",
            " 38 6611M   38 2564M    0     0  37.3M      0  0:02:56  0:01:08  0:01:48 27.9M\n",
            " 39 6611M   39 2588M    0     0  37.1M      0  0:02:57  0:01:09  0:01:48 27.3M\n",
            " 39 6611M   39 2612M    0     0  37.0M      0  0:02:58  0:01:10  0:01:48 26.6M\n",
            " 39 6611M   39 2638M    0     0  36.8M      0  0:02:59  0:01:11  0:01:48 25.9M\n",
            " 40 6611M   40 2664M    0     0  36.7M      0  0:03:00  0:01:12  0:01:48 25.1M\n",
            " 40 6611M   40 2690M    0     0  36.5M      0  0:03:00  0:01:13  0:01:47 25.2M\n",
            " 41 6611M   41 2717M    0     0  36.4M      0  0:03:01  0:01:14  0:01:47 25.8M\n",
            " 41 6611M   41 2743M    0     0  36.2M      0  0:03:02  0:01:15  0:01:47 26.1M\n",
            " 41 6611M   41 2769M    0     0  36.1M      0  0:03:02  0:01:16  0:01:46 26.2M\n",
            " 42 6611M   42 2796M    0     0  36.0M      0  0:03:03  0:01:17  0:01:46 26.3M\n",
            " 42 6611M   42 2822M    0     0  35.9M      0  0:03:04  0:01:18  0:01:46 26.3M\n",
            " 43 6611M   43 2849M    0     0  35.8M      0  0:03:04  0:01:19  0:01:45 26.5M\n",
            " 43 6611M   43 2877M    0     0  35.7M      0  0:03:05  0:01:20  0:01:45 26.8M\n",
            " 43 6611M   43 2906M    0     0  35.6M      0  0:03:05  0:01:21  0:01:44 27.3M\n",
            " 44 6611M   44 2937M    0     0  35.5M      0  0:03:05  0:01:22  0:01:43 28.2M\n",
            " 44 6611M   44 2969M    0     0  35.5M      0  0:03:06  0:01:23  0:01:43 29.4M\n",
            " 45 6611M   45 2995M    0     0  35.4M      0  0:03:06  0:01:24  0:01:42 29.1M\n",
            " 45 6611M   45 3023M    0     0  35.3M      0  0:03:07  0:01:25  0:01:42 29.1M\n",
            " 46 6611M   46 3053M    0     0  35.2M      0  0:03:07  0:01:26  0:01:41 29.3M\n",
            " 46 6611M   46 3085M    0     0  35.2M      0  0:03:07  0:01:27  0:01:40 29.5M\n",
            " 47 6611M   47 3117M    0     0  35.1M      0  0:03:07  0:01:28  0:01:39 29.4M\n",
            " 47 6611M   47 3139M    0     0  35.0M      0  0:03:08  0:01:29  0:01:39 28.8M\n",
            " 47 6611M   47 3157M    0     0  34.8M      0  0:03:09  0:01:30  0:01:39 26.8M\n",
            " 48 6611M   48 3177M    0     0  34.6M      0  0:03:10  0:01:31  0:01:39 24.7M\n",
            " 48 6611M   48 3197M    0     0  34.5M      0  0:03:11  0:01:32  0:01:39 22.4M\n",
            " 48 6611M   48 3217M    0     0  34.3M      0  0:03:12  0:01:33  0:01:39 20.1M\n",
            " 48 6611M   48 3238M    0     0  34.2M      0  0:03:13  0:01:34  0:01:39 19.8M\n",
            " 49 6611M   49 3259M    0     0  34.1M      0  0:03:13  0:01:35  0:01:38 20.4M\n",
            " 49 6611M   49 3280M    0     0  33.9M      0  0:03:14  0:01:36  0:01:38 20.6M\n",
            " 49 6611M   49 3301M    0     0  33.8M      0  0:03:15  0:01:37  0:01:38 20.8M\n",
            " 50 6611M   50 3322M    0     0  33.7M      0  0:03:16  0:01:38  0:01:38 20.9M\n",
            " 50 6611M   50 3343M    0     0  33.5M      0  0:03:16  0:01:39  0:01:37 21.0M\n",
            " 50 6611M   50 3365M    0     0  33.4M      0  0:03:17  0:01:40  0:01:37 21.2M\n",
            " 51 6611M   51 3388M    0     0  33.3M      0  0:03:18  0:01:41  0:01:37 21.5M\n",
            " 51 6611M   51 3412M    0     0  33.2M      0  0:03:18  0:01:42  0:01:36 22.1M\n",
            " 51 6611M   51 3438M    0     0  33.1M      0  0:03:19  0:01:43  0:01:36 23.0M\n",
            " 52 6611M   52 3462M    0     0  33.1M      0  0:03:19  0:01:44  0:01:35 23.7M\n",
            " 52 6611M   52 3484M    0     0  33.0M      0  0:03:20  0:01:45  0:01:35 23.7M\n",
            " 53 6611M   53 3508M    0     0  32.9M      0  0:03:20  0:01:46  0:01:34 24.0M\n",
            " 53 6611M   53 3534M    0     0  32.8M      0  0:03:21  0:01:47  0:01:34 24.4M\n",
            " 53 6611M   53 3561M    0     0  32.7M      0  0:03:21  0:01:48  0:01:33 24.6M\n",
            " 54 6611M   54 3588M    0     0  32.7M      0  0:03:21  0:01:49  0:01:32 25.1M\n",
            " 54 6611M   54 3616M    0     0  32.7M      0  0:03:22  0:01:50  0:01:32 26.3M\n",
            " 55 6611M   55 3638M    0     0  32.6M      0  0:03:22  0:01:51  0:01:31 26.0M\n",
            " 55 6611M   55 3660M    0     0  32.5M      0  0:03:23  0:01:52  0:01:31 25.2M\n",
            " 55 6611M   55 3683M    0     0  32.4M      0  0:03:23  0:01:53  0:01:30 24.3M\n",
            " 56 6611M   56 3706M    0     0  32.3M      0  0:03:24  0:01:54  0:01:30 23.5M\n",
            " 56 6611M   56 3729M    0     0  32.2M      0  0:03:24  0:01:55  0:01:29 22.7M\n",
            " 56 6611M   56 3753M    0     0  32.1M      0  0:03:25  0:01:56  0:01:29 22.9M\n",
            " 57 6611M   57 3777M    0     0  32.1M      0  0:03:25  0:01:57  0:01:28 23.4M\n",
            " 57 6611M   57 3801M    0     0  32.0M      0  0:03:26  0:01:58  0:01:28 23.7M\n",
            " 57 6611M   57 3825M    0     0  31.9M      0  0:03:26  0:01:59  0:01:27 23.9M\n",
            " 58 6611M   58 3849M    0     0  31.9M      0  0:03:27  0:02:00  0:01:27 23.9M\n",
            " 58 6611M   58 3874M    0     0  31.8M      0  0:03:27  0:02:01  0:01:26 24.0M\n",
            " 58 6611M   58 3899M    0     0  31.8M      0  0:03:27  0:02:02  0:01:25 24.2M\n",
            " 59 6611M   59 3924M    0     0  31.7M      0  0:03:28  0:02:03  0:01:25 24.6M\n",
            " 59 6611M   59 3952M    0     0  31.7M      0  0:03:28  0:02:04  0:01:24 25.2M\n",
            " 60 6611M   60 3980M    0     0  31.6M      0  0:03:28  0:02:05  0:01:23 26.2M\n",
            " 60 6611M   60 4010M    0     0  31.6M      0  0:03:28  0:02:06  0:01:22 27.3M\n",
            " 61 6611M   61 4035M    0     0  31.6M      0  0:03:29  0:02:07  0:01:22 27.2M\n",
            " 61 6611M   61 4062M    0     0  31.5M      0  0:03:29  0:02:08  0:01:21 27.5M\n",
            " 61 6611M   61 4091M    0     0  31.5M      0  0:03:29  0:02:09  0:01:20 27.8M\n",
            " 62 6611M   62 4121M    0     0  31.5M      0  0:03:29  0:02:10  0:01:19 28.0M\n",
            " 62 6611M   62 4152M    0     0  31.5M      0  0:03:29  0:02:11  0:01:18 28.2M\n",
            " 63 6611M   63 4178M    0     0  31.5M      0  0:03:29  0:02:12  0:01:17 28.6M\n",
            " 63 6611M   63 4202M    0     0  31.4M      0  0:03:30  0:02:13  0:01:17 28.0M\n",
            " 63 6611M   63 4227M    0     0  31.4M      0  0:03:30  0:02:14  0:01:16 27.3M\n",
            " 64 6611M   64 4253M    0     0  31.3M      0  0:03:30  0:02:15  0:01:15 26.5M\n",
            " 64 6611M   64 4280M    0     0  31.3M      0  0:03:30  0:02:16  0:01:14 25.6M\n",
            " 65 6611M   65 4307M    0     0  31.3M      0  0:03:31  0:02:17  0:01:14 25.8M\n",
            " 65 6611M   65 4328M    0     0  31.2M      0  0:03:31  0:02:18  0:01:13 25.2M\n",
            " 65 6611M   65 4350M    0     0  31.1M      0  0:03:32  0:02:19  0:01:13 24.4M\n",
            " 66 6611M   66 4373M    0     0  31.1M      0  0:03:32  0:02:20  0:01:12 23.8M\n",
            " 66 6611M   66 4398M    0     0  31.0M      0  0:03:32  0:02:21  0:01:11 23.5M\n",
            " 66 6611M   66 4423M    0     0  31.0M      0  0:03:33  0:02:22  0:01:11 23.2M\n",
            " 67 6611M   67 4449M    0     0  30.9M      0  0:03:33  0:02:23  0:01:10 24.2M\n",
            " 67 6611M   67 4474M    0     0  30.9M      0  0:03:33  0:02:24  0:01:09 24.7M\n",
            " 67 6611M   67 4494M    0     0  30.8M      0  0:03:34  0:02:25  0:01:09 24.2M\n",
            " 68 6611M   68 4515M    0     0  30.8M      0  0:03:34  0:02:26  0:01:08 23.5M\n",
            " 68 6611M   68 4537M    0     0  30.7M      0  0:03:35  0:02:27  0:01:08 22.7M\n",
            " 68 6611M   68 4560M    0     0  30.6M      0  0:03:35  0:02:28  0:01:07 22.0M\n",
            " 69 6611M   69 4582M    0     0  30.6M      0  0:03:35  0:02:29  0:01:06 21.7M\n",
            " 69 6611M   69 4605M    0     0  30.5M      0  0:03:36  0:02:30  0:01:06 22.2M\n",
            " 70 6611M   70 4628M    0     0  30.5M      0  0:03:36  0:02:31  0:01:05 22.6M\n",
            " 70 6611M   70 4651M    0     0  30.4M      0  0:03:36  0:02:32  0:01:04 22.8M\n",
            " 70 6611M   70 4675M    0     0  30.4M      0  0:03:37  0:02:33  0:01:04 22.9M\n",
            " 71 6611M   71 4698M    0     0  30.3M      0  0:03:37  0:02:34  0:01:03 23.0M\n",
            " 71 6611M   71 4721M    0     0  30.3M      0  0:03:37  0:02:35  0:01:02 23.2M\n",
            " 71 6611M   71 4746M    0     0  30.3M      0  0:03:38  0:02:36  0:01:02 23.4M\n",
            " 72 6611M   72 4771M    0     0  30.2M      0  0:03:38  0:02:37  0:01:01 23.9M\n",
            " 72 6611M   72 4798M    0     0  30.2M      0  0:03:38  0:02:38  0:01:00 24.6M\n",
            " 73 6611M   73 4827M    0     0  30.2M      0  0:03:38  0:02:39  0:00:59 25.8M\n",
            " 73 6611M   73 4858M    0     0  30.2M      0  0:03:38  0:02:40  0:00:58 27.4M\n",
            " 74 6611M   74 4893M    0     0  30.2M      0  0:03:38  0:02:41  0:00:57 29.5M\n",
            " 74 6611M   74 4932M    0     0  30.3M      0  0:03:37  0:02:42  0:00:55 32.2M\n",
            " 75 6611M   75 4976M    0     0  30.4M      0  0:03:37  0:02:43  0:00:54 35.6M\n",
            " 76 6611M   76 5026M    0     0  30.5M      0  0:03:36  0:02:44  0:00:52 39.8M\n",
            " 76 6611M   76 5082M    0     0  30.6M      0  0:03:35  0:02:45  0:00:50 44.7M\n",
            " 77 6611M   77 5147M    0     0  30.8M      0  0:03:33  0:02:46  0:00:47 50.6M\n",
            " 78 6611M   78 5198M    0     0  31.0M      0  0:03:33  0:02:47  0:00:46 53.1M\n",
            " 79 6611M   79 5236M    0     0  31.0M      0  0:03:32  0:02:48  0:00:44 52.0M\n",
            " 79 6611M   79 5277M    0     0  31.1M      0  0:03:32  0:02:49  0:00:43 50.1M\n",
            " 80 6611M   80 5318M    0     0  31.1M      0  0:03:32  0:02:50  0:00:42 47.1M\n",
            " 81 6611M   81 5361M    0     0  31.2M      0  0:03:31  0:02:51  0:00:40 42.7M\n",
            " 81 6611M   81 5404M    0     0  31.3M      0  0:03:31  0:02:52  0:00:39 41.1M\n",
            " 82 6611M   82 5447M    0     0  31.3M      0  0:03:30  0:02:53  0:00:37 42.1M\n",
            " 83 6611M   83 5490M    0     0  31.4M      0  0:03:30  0:02:54  0:00:36 42.7M\n",
            " 83 6611M   83 5534M    0     0  31.5M      0  0:03:29  0:02:55  0:00:34 43.1M\n",
            " 84 6611M   84 5577M    0     0  31.5M      0  0:03:29  0:02:56  0:00:33 43.3M\n",
            " 85 6611M   85 5620M    0     0  31.6M      0  0:03:28  0:02:57  0:00:31 43.3M\n",
            " 85 6611M   85 5664M    0     0  31.7M      0  0:03:28  0:02:58  0:00:30 43.3M\n",
            " 86 6611M   86 5708M    0     0  31.7M      0  0:03:28  0:02:59  0:00:29 43.5M\n",
            " 87 6611M   87 5753M    0     0  31.8M      0  0:03:27  0:03:00  0:00:27 43.8M\n",
            " 87 6611M   87 5798M    0     0  31.9M      0  0:03:27  0:03:01  0:00:26 44.2M\n",
            " 88 6611M   88 5846M    0     0  32.0M      0  0:03:26  0:03:02  0:00:24 45.0M\n",
            " 89 6611M   89 5895M    0     0  32.1M      0  0:03:25  0:03:03  0:00:22 46.1M\n",
            " 89 6611M   89 5945M    0     0  32.2M      0  0:03:25  0:03:04  0:00:21 47.4M\n",
            " 90 6611M   90 5996M    0     0  32.3M      0  0:03:24  0:03:05  0:00:19 48.6M\n",
            " 91 6611M   91 6055M    0     0  32.4M      0  0:03:23  0:03:06  0:00:17 51.3M\n",
            " 92 6611M   92 6119M    0     0  32.6M      0  0:03:22  0:03:07  0:00:15 54.6M\n",
            " 93 6611M   93 6185M    0     0  32.7M      0  0:03:21  0:03:08  0:00:13 57.9M\n",
            " 94 6611M   94 6225M    0     0  32.8M      0  0:03:21  0:03:09  0:00:12 55.9M\n",
            " 94 6611M   94 6264M    0     0  32.8M      0  0:03:21  0:03:10  0:00:11 53.6M\n",
            " 95 6611M   95 6305M    0     0  32.9M      0  0:03:20  0:03:11  0:00:09 49.9M\n",
            " 95 6611M   95 6347M    0     0  32.9M      0  0:03:20  0:03:12  0:00:08 45.5M\n",
            " 96 6611M   96 6389M    0     0  33.0M      0  0:03:20  0:03:13  0:00:07 40.9M\n",
            " 97 6611M   97 6433M    0     0  33.0M      0  0:03:19  0:03:14  0:00:05 41.5M\n",
            " 97 6611M   97 6476M    0     0  33.1M      0  0:03:19  0:03:15  0:00:04 42.4M\n",
            " 98 6611M   98 6519M    0     0  33.1M      0  0:03:19  0:03:16  0:00:03 42.9M\n",
            " 99 6611M   99 6563M    0     0  33.2M      0  0:03:19  0:03:17  0:00:02 43.1M\n",
            " 99 6611M   99 6606M    0     0  33.2M      0  0:03:18  0:03:18 --:--:-- 43.3M\n",
            "100 6611M  100 6611M    0     0  33.2M      0  0:03:18  0:03:18 --:--:-- 43.4M\n",
            "'unrar' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'rm' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "# Download UCF-101 dataset and labels\n",
        "# Download data\n",
        "# !curl -L -o UCF101.rar https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n",
        "# !unrar x UCF101.rar\n",
        "# !rm UCF101.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_wj05777e9z",
        "outputId": "6d2f60de-3707-44c9-e6a2-5befe9355fb8"
      },
      "outputs": [],
      "source": [
        "# Download train & test split\n",
        "# !curl -L https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip -O UCF101TrainTestSplits-RecognitionTask.zip\n",
        "# !unzip -q UCF101TrainTestSplits-RecognitionTask.zip\n",
        "# !rm UCF101TrainTestSplits-RecognitionTask.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A0ydSNMV7jyA"
      },
      "outputs": [],
      "source": [
        "# !copy /b ./ucfTrainTestlist/testlist01.txt + ./ucfTrainTestlist/testlist02.txt + ./ucfTrainTestlist/testlist03.txt ./ucfTrainTestlist/testlist.txt\n",
        "# !copy /b ./ucfTrainTestlist/trainlist01.txt + ./ucfTrainTestlist/trainlist02.txt + ./ucfTrainTestlist/trainlist03.txt ./ucfTrainTestlist/trainlist.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CNS-57fq7luO"
      },
      "outputs": [],
      "source": [
        "UCF_CLASSES = ['ApplyEyeMakeup','ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'Hammering', 'HammerThrow', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpingJack', 'JumpRope', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZNsFHZe7mJZ",
        "outputId": "5e7b7640-9624-4c18-f5ed-5e38c764df52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  id  label\n",
            "0  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "1  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "2  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "3  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "5  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "Number of rows :  22998\n"
          ]
        }
      ],
      "source": [
        "ucf_train_df = pd.read_csv('ucfTrainTestlist/trainlist.txt', sep=' ', header=None)\n",
        "ucf_train_df.columns = ['id', 'label']\n",
        "\n",
        "ucf_valid_df = ucf_train_df.sample(frac=0.2)\n",
        "ucf_valid_df['id'] = ucf_valid_df['id'].apply(lambda x: f\"./UCF-101/UCF-101/{x}\")\n",
        "\n",
        "ucf_train_df = ucf_train_df.drop(ucf_valid_df.index)\n",
        "ucf_train_df['id'] = ucf_train_df['id'].apply(lambda x: f\"./UCF-101/UCF-101/{x}\")\n",
        "ucf_train_df['label'] = ucf_train_df['label'].apply(lambda x: x-1)\n",
        "ucf_valid_df['label'] = ucf_valid_df['label'].apply(lambda x: x-1)\n",
        "\n",
        "print(ucf_train_df.head())\n",
        "print(\"Number of rows : \", ucf_train_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmtCTa0E70yi",
        "outputId": "363acb6c-aa9f-4a1a-c1a5-5a0b4e6603b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  id  label\n",
            "0  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "1  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "2  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "3  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "4  ./UCF-101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMak...      0\n",
            "Number of rows :  11213\n"
          ]
        }
      ],
      "source": [
        "ucf_class_df = pd.read_csv('ucfTrainTestlist/classInd.txt', sep=' ', header=None)\n",
        "ucf_class_df.columns = ['label', 'label_name']\n",
        "\n",
        "ucf_test_df = pd.read_csv('ucfTrainTestlist/testlist.txt', sep=' ', header=None)\n",
        "ucf_test_df.columns = ['id']\n",
        "ucf_test_df['label'] = ucf_test_df['id'].str.split('/').str[0]\n",
        "\n",
        "label_mapping = dict(zip(ucf_class_df['label_name'], ucf_class_df['label']))\n",
        "ucf_test_df['label'] = ucf_test_df['label'].map(label_mapping)\n",
        "\n",
        "ucf_test_label_df = ucf_test_df[['id', 'label']]\n",
        "ucf_test_df = ucf_test_df.drop(columns=['label'])\n",
        "\n",
        "ucf_test_label_df['label'] = ucf_test_label_df['label'].apply(lambda x: x-1)\n",
        "ucf_test_label_df['id'] = ucf_test_label_df['id'].apply(lambda x: f\"./UCF-101/UCF-101/{x}\")\n",
        "\n",
        "# N_CALL_UCF = ucf_test_df['label'].nunique()\n",
        "\n",
        "print(ucf_test_label_df.head())\n",
        "print(\"Number of rows : \", ucf_test_label_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oX7uZHfY76ox"
      },
      "outputs": [],
      "source": [
        "def read_video_pyav(container, indices):\n",
        "    '''\n",
        "    ...     Decode the video with PyAV decoder.\n",
        "    ...     Args:\n",
        "    ...         container (`av.container.input.InputContainer`): PyAV container.\n",
        "    ...         indices (`List[int]`): List of frame indices to decode.\n",
        "    ...     Returns:\n",
        "    ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "    ...     '''\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            frames.append(frame)\n",
        "\n",
        "    if len(frames) == 0 :\n",
        "        pass\n",
        "\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KTB_ux1o79A1"
      },
      "outputs": [],
      "source": [
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    '''\n",
        "        ...     Sample a given number of frame indices from the video.\n",
        "        ...     Args:\n",
        "        ...         clip_len (`int`): Total number of frames to sample.\n",
        "        ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "        ...     Returns:\n",
        "        ...         indices (`List[int]`): List of sampled frame indices\n",
        "    '''\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "\n",
        "    while converted_len >= seg_len:\n",
        "        # You could either adjust clip_len or frame_sample_rate, or both\n",
        "        # For example, reduce clip_len to fit the available frames:\n",
        "        frame_sample_rate = seg_len // clip_len\n",
        "        # Recalculate converted_len based on the adjusted clip_len\n",
        "        converted_len = clip_len * frame_sample_rate\n",
        "\n",
        "\n",
        "        if converted_len == seg_len:\n",
        "            frame_sample_rate -= 1\n",
        "            converted_len = clip_len * frame_sample_rate\n",
        "\n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IVdx3ylE7-zb"
      },
      "outputs": [],
      "source": [
        "def format_video(video_path):\n",
        "    container = av.open(video_path)\n",
        "    seg_len = int(container.streams.video[0].frames)\n",
        "    indices = sample_frame_indices(clip_len=8, frame_sample_rate=4, seg_len=seg_len)\n",
        "    video = read_video_pyav(container, indices)\n",
        "    return video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8dPwgElE8HLC"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_data (list or np.array): Preprocessed image data, should be in shape (num_samples, height, width, channels).\n",
        "            labels (list or np.array): Labels corresponding to the images.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve the image and label at index `idx`\n",
        "        row = self.df.iloc[idx]\n",
        "        image = row['id']\n",
        "        label = int(row['label'])\n",
        "\n",
        "        # If your image needs to be converted to a torch tensor\n",
        "        # image = torch.tensor(image, dtype=torch.float32)  # Adjust dtype if necessary\n",
        "\n",
        "        # Depending on your label format, convert the label\n",
        "        label = torch.tensor(label, dtype=torch.long)  # Assuming it's a classification problem\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zetE6ucK8LWk"
      },
      "outputs": [],
      "source": [
        "data = [ucf_train_df, ucf_valid_df, ucf_test_label_df]\n",
        "\n",
        "# data = [train_df, validation_df, test_label_df]\n",
        "\n",
        "train_dataset = CustomImageDataset(data[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "valisation_dataset = CustomImageDataset(data[1])\n",
        "val_loader = DataLoader(valisation_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "evaluation_dataset = CustomImageDataset(data[2])\n",
        "evaluation_loader = DataLoader(evaluation_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-eY7DWeQ8NGh"
      },
      "outputs": [],
      "source": [
        "def evaluation_run(model, image_processor, criterion, evaluation_set):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for vid_id, labels in tqdm.tqdm(evaluation_set):\n",
        "            vid_id = vid_id[0]\n",
        "\n",
        "            try:\n",
        "                images = format_video(vid_id)\n",
        "\n",
        "                images = torch.tensor(images, dtype=torch.float32)\n",
        "\n",
        "                images = torch.squeeze(images)\n",
        "                inputs = image_processor(list(images), return_tensors=\"pt\")\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs['logits']\n",
        "\n",
        "            except Exception as e:\n",
        "                total += 1 \n",
        "                continue\n",
        "\n",
        "\n",
        "            running_loss += criterion(logits, labels)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total += 1\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    validation_loss = running_loss / len(evaluation_set)\n",
        "    accuracy = (100 * correct) / total\n",
        "    print(f\"Evaluation : Loss: {validation_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    return validation_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97PH_7ui8O0U"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "def validation_run(model, image_processor, criterion, validation_set):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for vid_id, labels in tqdm.tqdm(validation_set):\n",
        "            vid_id = vid_id[0]\n",
        "\n",
        "            try:\n",
        "                images = format_video(vid_id)\n",
        "\n",
        "                images = torch.tensor(images, dtype=torch.float32)\n",
        "\n",
        "                images = torch.squeeze(images)\n",
        "                inputs = image_processor(list(images), return_tensors=\"pt\")\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs['logits']\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "            running_loss += criterion(logits, labels)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total += 1\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    validation_loss = running_loss / len(validation_set)\n",
        "    accuracy = (100 * correct) / total\n",
        "    print(f\"Validation : Loss: {validation_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    return validation_loss\n",
        "\n",
        "def train_model(model, image_processor, training_dataloader, criterion, optimizer, num_epochs=10, validation_dataloader=val_loader):\n",
        "    validation_loss = []\n",
        "    training_loss = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        i = 1\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        loss = 0\n",
        "        model.train(True)\n",
        "\n",
        "        for vid_id, labels in tqdm.tqdm(training_dataloader):\n",
        "            vid_id = vid_id[0]\n",
        "\n",
        "            try:\n",
        "                images = format_video(vid_id)\n",
        "\n",
        "                images = torch.tensor(images, dtype=torch.float32)\n",
        "                images = torch.squeeze(images)\n",
        "\n",
        "                inputs = image_processor(list(images), return_tensors=\"pt\")\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs['logits']\n",
        "            except Exception as e:\n",
        "                total += 1\n",
        "                torch.cuda.empty_cache()\n",
        "                # print(e)\n",
        "                continue\n",
        "            \n",
        "            # Calculer la perte\n",
        "            loss = criterion(logits, labels) / BATCH_SIZE\n",
        "            loss.backward()\n",
        "\n",
        "            # Rétropropagation de la perte\n",
        "            if (i+1) % BATCH_SIZE == 0:\n",
        "              # Mettre à jour les paramètres du modèle\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "            i += 1\n",
        "\n",
        "            # Calcul des statistiques\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total += 1\n",
        "            try:\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Affichage des statistiques après chaque époque\n",
        "        epoch_loss = running_loss / len(training_dataloader)\n",
        "        accuracy = (100 * correct) / total\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "        val_loss = validation_run(model, image_processor, criterion, validation_dataloader)\n",
        "\n",
        "        validation_loss.append(val_loss)\n",
        "        training_loss.append(epoch_loss)\n",
        "\n",
        "        torch.save(model.state_dict(), f\"./results/training_{TRAINING}/weights_epoch%d.pt\"%epoch)\n",
        "\n",
        "    return training_loss, validation_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.modules.pop('implementations.llora_timesformer', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7WLWTwy8Qcd",
        "outputId": "72d315e0-56e9-4d35-ccd5-193d309bf396"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\calorrain\\AppData\\Local\\anaconda3\\envs\\visu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized: ['timesformer.encoder.layer.0.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.0.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.0.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.0.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.0.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.0.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.0.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.0.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.1.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.1.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.1.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.1.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.1.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.1.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.1.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.1.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.10.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.10.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.10.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.10.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.10.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.10.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.10.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.10.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.11.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.11.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.11.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.11.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.11.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.11.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.11.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.11.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.2.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.2.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.2.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.2.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.2.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.2.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.2.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.2.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.3.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.3.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.3.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.3.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.3.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.3.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.3.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.3.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.4.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.4.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.4.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.4.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.4.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.4.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.4.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.4.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.5.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.5.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.5.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.5.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.5.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.5.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.5.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.5.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.6.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.6.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.6.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.6.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.6.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.6.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.6.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.6.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.7.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.7.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.7.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.7.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.7.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.7.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.7.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.7.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.8.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.8.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.8.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.8.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.8.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.8.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.8.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.8.temporal_attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.9.attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.9.attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.9.attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.9.attention.attention.prefix_v.embeddings.weight', 'timesformer.encoder.layer.9.temporal_attention.attention.prefix_k.embeddings.bias', 'timesformer.encoder.layer.9.temporal_attention.attention.prefix_k.embeddings.weight', 'timesformer.encoder.layer.9.temporal_attention.attention.prefix_v.embeddings.bias', 'timesformer.encoder.layer.9.temporal_attention.attention.prefix_v.embeddings.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([102, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([102]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TimesformerForVideoClassification(\n",
              "  (timesformer): TimesformerModel(\n",
              "    (embeddings): TimesformerEmbeddings(\n",
              "      (patch_embeddings): TimesformerPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (time_drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): TimesformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x TimesformerLayer(\n",
              "          (drop_path): Identity()\n",
              "          (attention): TimeSformerAttention(\n",
              "            (attention): TimesformerSelfAttention(\n",
              "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (prefix_k): PrefixModule(\n",
              "                (embeddings): Linear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (prefix_v): PrefixModule(\n",
              "                (embeddings): Linear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (output): TimesformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): TimesformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): TimesformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (temporal_attention): TimeSformerAttention(\n",
              "            (attention): TimesformerSelfAttention(\n",
              "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (prefix_k): PrefixModule(\n",
              "                (embeddings): Linear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (prefix_v): PrefixModule(\n",
              "                (embeddings): Linear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (output): TimesformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=102, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "import importlib\n",
        "llora_mod = importlib.import_module('implementations.prefix_timesformer')\n",
        "importlib.reload(llora_mod)\n",
        "TimesformerForVideoClassification = llora_mod.TimesformerForVideoClassification\n",
        "\n",
        "TRAINING = 'PREFIX' # Modify this value each run\n",
        "# os.mkdir(f\"./results/training_{TRAINING}\")\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\", num_labels=len(UCF_CLASSES)+1, ignore_mismatched_sizes=True)\n",
        "model.train(True)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeJzefvK8S_G",
        "outputId": "4866f9ce-32dc-4948-b0d7-da7d94d65f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "timesformer.encoder.layer.0.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.0.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.0.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.0.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.0.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.0.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.0.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.0.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.1.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.1.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.1.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.1.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.1.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.1.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.1.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.1.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.2.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.2.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.2.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.2.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.2.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.2.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.2.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.2.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.3.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.3.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.3.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.3.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.3.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.3.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.3.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.3.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.4.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.4.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.4.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.4.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.4.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.4.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.4.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.4.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.5.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.5.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.5.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.5.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.5.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.5.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.5.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.5.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.6.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.6.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.6.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.6.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.6.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.6.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.6.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.6.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.7.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.7.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.7.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.7.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.7.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.7.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.7.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.7.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.8.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.8.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.8.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.8.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.8.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.8.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.8.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.8.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.9.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.9.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.9.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.9.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.9.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.9.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.9.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.9.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.10.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.10.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.10.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.10.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.10.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.10.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.10.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.10.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.11.attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.11.attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.11.attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.11.attention.attention.prefix_v.embeddings.bias\n",
            "timesformer.encoder.layer.11.temporal_attention.attention.prefix_k.embeddings.weight\n",
            "timesformer.encoder.layer.11.temporal_attention.attention.prefix_k.embeddings.bias\n",
            "timesformer.encoder.layer.11.temporal_attention.attention.prefix_v.embeddings.weight\n",
            "timesformer.encoder.layer.11.temporal_attention.attention.prefix_v.embeddings.bias\n",
            "classifier.weight\n",
            "classifier.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if 'prefix' not in name and 'classifier' not in name:\n",
        "        param.requires_grad = False\n",
        "    else :\n",
        "        print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgUeag4-8UKU",
        "outputId": "0c0b2331-9c28-46c8-abe2-9ff81b9bdbe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28426854\n"
          ]
        }
      ],
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X5pEy89C8VYE"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Set the scheduler to decay the LR by 10x at epochs 11 and 14\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[11, 14], gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t9UVYZk8YDS",
        "outputId": "e85b39e5-8a77-4eb7-880d-66475a3e78ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [48:22<00:00,  7.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 0.0532, Accuracy: 86.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:12<00:00, 13.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0764, Accuracy: 99.01%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|████▉     | 11406/22998 [23:59<23:54,  8.08it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
            "100%|██████████| 22998/22998 [48:24<00:00,  7.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 0.0023, Accuracy: 99.51%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:12<00:00, 13.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0308, Accuracy: 99.39%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [48:41<00:00,  7.87it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/15], Loss: 0.0009, Accuracy: 99.82%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:12<00:00, 13.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0139, Accuracy: 99.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [48:23<00:00,  7.92it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/15], Loss: 0.0004, Accuracy: 99.89%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:11<00:00, 13.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0174, Accuracy: 99.60%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [48:25<00:00,  7.91it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/15], Loss: 0.0004, Accuracy: 99.87%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:07<00:00, 13.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0120, Accuracy: 99.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:07<00:00,  7.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/15], Loss: 0.0003, Accuracy: 99.93%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:25<00:00, 12.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0097, Accuracy: 99.77%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [48:58<00:00,  7.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/15], Loss: 0.0004, Accuracy: 99.83%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:34<00:00, 12.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0114, Accuracy: 99.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:18<00:00,  7.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/15], Loss: 0.0002, Accuracy: 99.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:28<00:00, 12.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0042, Accuracy: 99.91%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:18<00:00,  7.77it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/15], Loss: 0.0002, Accuracy: 99.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:32<00:00, 12.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0069, Accuracy: 99.77%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:15<00:00,  7.78it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/15], Loss: 0.0001, Accuracy: 99.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:28<00:00, 12.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0044, Accuracy: 99.88%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:17<00:00,  7.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/15], Loss: 0.0002, Accuracy: 99.94%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:27<00:00, 12.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0136, Accuracy: 99.62%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:13<00:00,  7.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/15], Loss: 0.0001, Accuracy: 99.96%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:30<00:00, 12.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0131, Accuracy: 99.60%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:17<00:00,  7.78it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/15], Loss: 0.0001, Accuracy: 99.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:28<00:00, 12.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0059, Accuracy: 99.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:06<00:00,  7.81it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/15], Loss: 0.0002, Accuracy: 99.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:29<00:00, 12.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0082, Accuracy: 99.81%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22998/22998 [49:12<00:00,  7.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/15], Loss: 0.0002, Accuracy: 99.94%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5749/5749 [07:32<00:00, 12.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 0.0033, Accuracy: 99.91%\n"
          ]
        }
      ],
      "source": [
        "validation_loss, training_loss = train_model(model, image_processor, train_loader, criterion, optimizer, num_epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mazSt6Ft8ZcZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11213/11213 [14:30<00:00, 12.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation : Loss: 0.0017, Accuracy: 99.93%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluation_loss = evaluation_run(model, image_processor, criterion, evaluation_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkyRjyp9ADu6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "visu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
