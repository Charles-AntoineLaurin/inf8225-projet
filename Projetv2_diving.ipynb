{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkCAx1Oa7S-D",
        "outputId": "55f7472b-c2b5-4ad9-c77c-38612ccdb7f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\calorrain\\appdata\\local\\anaconda3\\envs\\visu\\lib\\site-packages (from scikit-learn) (1.23.4)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "# !pip install av\n",
        "# !pip install matplotlib \n",
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XU_-YcBz7Yf4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import av\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ksLuqkqm7ZDw"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# os.mkdir('results')\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq0RsySA7c1b",
        "outputId": "5e72588d-5284-4786-fd10-fd79fd483e2c"
      },
      "outputs": [],
      "source": [
        "# Download UCF-101 dataset and labels\n",
        "# Download data\n",
        "# !curl -L -o Diving48_rgb.tar.gz https://nextcloud.nrp-nautilus.io/s/eqKMRFHqNCrP77L/download/Diving48_rgb.tar.gz\n",
        "# !unrar x UCF101.rar\n",
        "# !rm UCF101.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_wj05777e9z",
        "outputId": "6d2f60de-3707-44c9-e6a2-5befe9355fb8"
      },
      "outputs": [],
      "source": [
        "# Download train & test split\n",
        "# !curl -L https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip -O UCF101TrainTestSplits-RecognitionTask.zip\n",
        "# !unzip -q UCF101TrainTestSplits-RecognitionTask.zip\n",
        "# !rm UCF101TrainTestSplits-RecognitionTask.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A0ydSNMV7jyA"
      },
      "outputs": [],
      "source": [
        "# !copy /b ./ucfTrainTestlist/testlist01.txt + ./ucfTrainTestlist/testlist02.txt + ./ucfTrainTestlist/testlist03.txt ./ucfTrainTestlist/testlist.txt\n",
        "# !copy /b ./ucfTrainTestlist/trainlist01.txt + ./ucfTrainTestlist/trainlist02.txt + ./ucfTrainTestlist/trainlist03.txt ./ucfTrainTestlist/trainlist.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CNS-57fq7luO"
      },
      "outputs": [],
      "source": [
        "# UCF_CLASSES = ['ApplyEyeMakeup','ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'Hammering', 'HammerThrow', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpingJack', 'JumpRope', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZNsFHZe7mJZ",
        "outputId": "5e7b7640-9624-4c18-f5ed-5e38c764df52"
      },
      "outputs": [],
      "source": [
        "# ucf_train_df = pd.read_csv('ucfTrainTestlist/trainlist.txt', sep=' ', header=None)\n",
        "# ucf_train_df.columns = ['id', 'label']\n",
        "\n",
        "# ucf_valid_df = ucf_train_df.sample(frac=0.2)\n",
        "# ucf_valid_df['id'] = ucf_valid_df['id'].apply(lambda x: f\"./UCF-101/UCF-101/{x}\")\n",
        "\n",
        "# ucf_train_df = ucf_train_df.drop(ucf_valid_df.index)\n",
        "# ucf_train_df['id'] = ucf_train_df['id'].apply(lambda x: f\"./UCF-101/UCF-101/{x}\")\n",
        "# ucf_train_df['label'] = ucf_train_df['label'].apply(lambda x: x-1)\n",
        "# ucf_valid_df['label'] = ucf_valid_df['label'].apply(lambda x: x-1)\n",
        "\n",
        "# print(ucf_train_df.head())\n",
        "# print(\"Number of rows : \", ucf_train_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                 id  label\n",
            "14311  .\\diving48\\diving48\\19/siEI_jaSmd4_00081.mp4     34\n",
            "9870   .\\diving48\\diving48\\22/Bb0ZiYVNtDs_00199.mp4     22\n",
            "553    .\\diving48\\diving48\\19/VNvb5oLOpLg_00570.mp4      2\n",
            "8116   .\\diving48\\diving48\\21/sk8TafuB3lU_01024.mp4     12\n",
            "7248   .\\diving48\\diving48\\15/sk8TafuB3lU_00109.mp4     15\n",
            "Number of rows :  12021\n"
          ]
        }
      ],
      "source": [
        "diving_train_df = pd.read_csv('./diving48_train_list_videos.txt', sep=' ', header=None)\n",
        "diving_train_df.columns = ['id', 'label']\n",
        "\n",
        "diving_train_df['id'] = diving_train_df['id'].apply(lambda x: f\".\\\\diving48\\\\diving48\\\\{x}\")\n",
        "\n",
        "diving_train_df, diving_valid_df = train_test_split(diving_train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(diving_train_df.head())\n",
        "print(\"Number of rows : \", diving_train_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                 id  label\n",
            "2919   .\\diving48\\diving48\\35/nOlRwoxsDJ0_00761.mp4     26\n",
            "7414    .\\diving48\\diving48\\1/sk8TafuB3lU_00295.mp4     17\n",
            "3344   .\\diving48\\diving48\\34/3N1kUtqJ25A_00004.mp4     34\n",
            "11010  .\\diving48\\diving48\\46/9jZYYtzYqwE_00021.mp4     46\n",
            "8582   .\\diving48\\diving48\\35/xbQCwTHcGN8_00007.mp4     35\n",
            "Number of rows :  3006\n"
          ]
        }
      ],
      "source": [
        "print(diving_valid_df.head())\n",
        "print(\"Number of rows : \", diving_valid_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             id  label\n",
            "0  ./diving48/diving48/26/rRw7peH60Yw_00000.mp4     26\n",
            "1  ./diving48/diving48/33/rRw7peH60Yw_00001.mp4     33\n",
            "2  ./diving48/diving48/27/rRw7peH60Yw_00002.mp4     27\n",
            "3  ./diving48/diving48/33/rRw7peH60Yw_00003.mp4     33\n",
            "4  ./diving48/diving48/26/rRw7peH60Yw_00004.mp4     26\n",
            "Number of rows :  1970\n"
          ]
        }
      ],
      "source": [
        "diving_test_df = pd.read_csv('./diving48_val_list_videos.txt', sep=' ', header=None)\n",
        "diving_test_df.columns = ['id', 'label']\n",
        "\n",
        "diving_test_df['id'] = diving_test_df['id'].apply(lambda x: f\"./diving48/diving48/{x}\")\n",
        "\n",
        "print(diving_test_df.head())\n",
        "print(\"Number of rows : \", diving_test_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmtCTa0E70yi",
        "outputId": "363acb6c-aa9f-4a1a-c1a5-5a0b4e6603b5"
      },
      "outputs": [],
      "source": [
        "# ucf_class_df = pd.read_csv('ucfTrainTestlist/classInd.txt', sep=' ', header=None)\n",
        "# ucf_class_df.columns = ['label', 'label_name']\n",
        "\n",
        "# ucf_test_df = pd.read_csv('ucfTrainTestlist/testlist.txt', sep=' ', header=None)\n",
        "# ucf_test_df.columns = ['id']\n",
        "# ucf_test_df['label'] = ucf_test_df['id'].str.split('/').str[0]\n",
        "\n",
        "# label_mapping = dict(zip(ucf_class_df['label_name'], ucf_class_df['label']))\n",
        "# ucf_test_df['label'] = ucf_test_df['label'].map(label_mapping)\n",
        "\n",
        "# ucf_test_label_df = ucf_test_df[['id', 'label']]\n",
        "# ucf_test_df = ucf_test_df.drop(columns=['label'])\n",
        "\n",
        "# ucf_test_label_df['label'] = ucf_test_label_df['label'].apply(lambda x: x-1)\n",
        "# ucf_test_label_df['id'] = ucf_test_label_df['id'].apply(lambda x: f\"./UCF-101/UCF-101/{x}\")\n",
        "\n",
        "# # N_CALL_UCF = ucf_test_df['label'].nunique()\n",
        "\n",
        "# print(ucf_test_label_df.head())\n",
        "# print(\"Number of rows : \", ucf_test_label_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oX7uZHfY76ox"
      },
      "outputs": [],
      "source": [
        "def read_video_pyav(container, indices):\n",
        "    '''\n",
        "    ...     Decode the video with PyAV decoder.\n",
        "    ...     Args:\n",
        "    ...         container (`av.container.input.InputContainer`): PyAV container.\n",
        "    ...         indices (`List[int]`): List of frame indices to decode.\n",
        "    ...     Returns:\n",
        "    ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "    ...     '''\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            frames.append(frame)\n",
        "\n",
        "    if len(frames) == 0 :\n",
        "        pass\n",
        "\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KTB_ux1o79A1"
      },
      "outputs": [],
      "source": [
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    '''\n",
        "        ...     Sample a given number of frame indices from the video.\n",
        "        ...     Args:\n",
        "        ...         clip_len (`int`): Total number of frames to sample.\n",
        "        ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "        ...     Returns:\n",
        "        ...         indices (`List[int]`): List of sampled frame indices\n",
        "    '''\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "\n",
        "    while converted_len >= seg_len:\n",
        "        # You could either adjust clip_len or frame_sample_rate, or both\n",
        "        # For example, reduce clip_len to fit the available frames:\n",
        "        frame_sample_rate = seg_len // clip_len\n",
        "        # Recalculate converted_len based on the adjusted clip_len\n",
        "        converted_len = clip_len * frame_sample_rate\n",
        "\n",
        "\n",
        "        if converted_len == seg_len:\n",
        "            frame_sample_rate -= 1\n",
        "            converted_len = clip_len * frame_sample_rate\n",
        "\n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IVdx3ylE7-zb"
      },
      "outputs": [],
      "source": [
        "def format_video(video_path):\n",
        "    container = av.open(video_path)\n",
        "    seg_len = int(container.streams.video[0].frames)\n",
        "    indices = sample_frame_indices(clip_len=8, frame_sample_rate=4, seg_len=seg_len)\n",
        "    video = read_video_pyav(container, indices)\n",
        "    return video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8dPwgElE8HLC"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_data (list or np.array): Preprocessed image data, should be in shape (num_samples, height, width, channels).\n",
        "            labels (list or np.array): Labels corresponding to the images.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve the image and label at index `idx`\n",
        "        row = self.df.iloc[idx]\n",
        "        image = row['id']\n",
        "        label = int(row['label'])\n",
        "\n",
        "        # If your image needs to be converted to a torch tensor\n",
        "        # image = torch.tensor(image, dtype=torch.float32)  # Adjust dtype if necessary\n",
        "\n",
        "        # Depending on your label format, convert the label\n",
        "        label = torch.tensor(label, dtype=torch.long)  # Assuming it's a classification problem\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zetE6ucK8LWk"
      },
      "outputs": [],
      "source": [
        "# data = [ucf_train_df, ucf_valid_df, ucf_test_label_df]\n",
        "\n",
        "data = [diving_train_df, diving_valid_df, diving_test_df]\n",
        "\n",
        "train_dataset = CustomImageDataset(data[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "valisation_dataset = CustomImageDataset(data[1])\n",
        "val_loader = DataLoader(valisation_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "evaluation_dataset = CustomImageDataset(data[2])\n",
        "evaluation_loader = DataLoader(evaluation_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-eY7DWeQ8NGh"
      },
      "outputs": [],
      "source": [
        "def evaluation_run(model, image_processor, criterion, evaluation_set):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for vid_id, labels in tqdm.tqdm(evaluation_set):\n",
        "            vid_id = vid_id[0]\n",
        "\n",
        "            try:\n",
        "                images = format_video(vid_id)\n",
        "\n",
        "                images = torch.tensor(images, dtype=torch.float32)\n",
        "\n",
        "                images = torch.squeeze(images)\n",
        "                inputs = image_processor(list(images), return_tensors=\"pt\")\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs['logits']\n",
        "\n",
        "            except Exception as e:\n",
        "                total += 1 \n",
        "                continue\n",
        "\n",
        "\n",
        "            running_loss += criterion(logits, labels)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total += 1\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    validation_loss = running_loss / len(evaluation_set)\n",
        "    accuracy = (100 * correct) / total\n",
        "    print(f\"Evaluation : Loss: {validation_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    return validation_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "97PH_7ui8O0U"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "def validation_run(model, image_processor, criterion, validation_set):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for vid_id, labels in tqdm.tqdm(validation_set):\n",
        "            vid_id = vid_id[0]\n",
        "\n",
        "            try:\n",
        "                images = format_video(vid_id)\n",
        "\n",
        "                images = torch.tensor(images, dtype=torch.float32)\n",
        "\n",
        "                images = torch.squeeze(images)\n",
        "                inputs = image_processor(list(images), return_tensors=\"pt\")\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs['logits']\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "            running_loss += criterion(logits, labels)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total += 1\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    validation_loss = running_loss / len(validation_set)\n",
        "    accuracy = (100 * correct) / total\n",
        "    print(f\"Validation : Loss: {validation_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    return validation_loss\n",
        "\n",
        "def train_model(model, image_processor, training_dataloader, criterion, optimizer, num_epochs=10, validation_dataloader=val_loader):\n",
        "    validation_loss = []\n",
        "    training_loss = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        i = 1\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        loss = 0\n",
        "        model.train(True)\n",
        "\n",
        "        for vid_id, labels in tqdm.tqdm(training_dataloader):\n",
        "            vid_id = vid_id[0]\n",
        "\n",
        "            try:\n",
        "                images = format_video(vid_id)\n",
        "\n",
        "                images = torch.tensor(images, dtype=torch.float32)\n",
        "                images = torch.squeeze(images)\n",
        "\n",
        "                inputs = image_processor(list(images), return_tensors=\"pt\")\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs['logits']\n",
        "            except Exception as e:\n",
        "                total += 1\n",
        "                torch.cuda.empty_cache()\n",
        "                # print(e)\n",
        "                continue\n",
        "            \n",
        "            # Calculer la perte\n",
        "            loss = criterion(logits, labels) / BATCH_SIZE\n",
        "            loss.backward()\n",
        "\n",
        "            # Rétropropagation de la perte\n",
        "            if (i+1) % BATCH_SIZE == 0:\n",
        "              # Mettre à jour les paramètres du modèle\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "            i += 1\n",
        "\n",
        "            # Calcul des statistiques\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total += 1\n",
        "            try:\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Affichage des statistiques après chaque époque\n",
        "        epoch_loss = running_loss / len(training_dataloader)\n",
        "        accuracy = (100 * correct) / total\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "        val_loss = validation_run(model, image_processor, criterion, validation_dataloader)\n",
        "\n",
        "        validation_loss.append(val_loss)\n",
        "        training_loss.append(epoch_loss)\n",
        "\n",
        "        torch.save(model.state_dict(), f\"./results/training_{TRAINING}/weights_epoch%d.pt\"%epoch)\n",
        "\n",
        "    return training_loss, validation_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.modules.pop('implementations.llora_timesformer', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7WLWTwy8Qcd",
        "outputId": "72d315e0-56e9-4d35-ccd5-193d309bf396"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([48, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([48]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TimesformerForVideoClassification(\n",
              "  (timesformer): TimesformerModel(\n",
              "    (embeddings): TimesformerEmbeddings(\n",
              "      (patch_embeddings): TimesformerPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (time_drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): TimesformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x TimesformerLayer(\n",
              "          (drop_path): Identity()\n",
              "          (attention): TimeSformerAttention(\n",
              "            (attention): TimesformerSelfAttention(\n",
              "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): TimesformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): TimesformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): TimesformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (temporal_attention): TimeSformerAttention(\n",
              "            (attention): TimesformerSelfAttention(\n",
              "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): TimesformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=48, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoImageProcessor,TimesformerForVideoClassification\n",
        "\n",
        "# import importlib\n",
        "# impl_mod = importlib.import_module('implementations.prefix_timesformer')\n",
        "# importlib.reload(impl_mod)\n",
        "# TimesformerForVideoClassification = impl_mod.TimesformerForVideoClassification\n",
        "\n",
        "TRAINING = 'DIVING_CLASSIFIER' # Modify this value each run\n",
        "# os.mkdir(f\"./results/training_{TRAINING}\")\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\", num_labels=48, ignore_mismatched_sizes=True)\n",
        "model.train(True)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeJzefvK8S_G",
        "outputId": "4866f9ce-32dc-4948-b0d7-da7d94d65f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classifier.weight\n",
            "classifier.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    # if 'prefix' not in name and 'classifier' not in name:\n",
        "    if 'classifier' not in name:\n",
        "        param.requires_grad = False\n",
        "    else :\n",
        "        print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgUeag4-8UKU",
        "outputId": "0c0b2331-9c28-46c8-abe2-9ff81b9bdbe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36912\n"
          ]
        }
      ],
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "X5pEy89C8VYE"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# Set the scheduler to decay the LR by 10x at epochs 11 and 14\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[], gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t9UVYZk8YDS",
        "outputId": "e85b39e5-8a77-4eb7-880d-66475a3e78ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12021/12021 [25:58<00:00,  7.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 0.2286, Accuracy: 18.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 2222/3006 [04:45<01:40,  7.81it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
            "100%|██████████| 3006/3006 [06:26<00:00,  7.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 3.6006, Accuracy: 19.96%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12021/12021 [32:25<00:00,  6.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5], Loss: 0.2061, Accuracy: 24.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3006/3006 [08:40<00:00,  5.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 3.2832, Accuracy: 27.18%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12021/12021 [28:27<00:00,  7.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5], Loss: 0.1975, Accuracy: 28.35%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3006/3006 [07:12<00:00,  6.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 3.7449, Accuracy: 26.11%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12021/12021 [29:59<00:00,  6.68it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5], Loss: 0.1969, Accuracy: 30.10%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3006/3006 [07:11<00:00,  6.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 3.5003, Accuracy: 27.15%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12021/12021 [30:25<00:00,  6.58it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5], Loss: 0.1912, Accuracy: 31.88%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3006/3006 [07:07<00:00,  7.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation : Loss: 3.4961, Accuracy: 29.08%\n"
          ]
        }
      ],
      "source": [
        "validation_loss, training_loss = train_model(model, image_processor, train_loader, criterion, optimizer, num_epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mazSt6Ft8ZcZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1970/1970 [04:41<00:00,  7.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation : Loss: 4.6691, Accuracy: 16.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluation_loss = evaluation_run(model, image_processor, criterion, evaluation_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkyRjyp9ADu6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "visu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
